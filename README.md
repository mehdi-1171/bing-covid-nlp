# ğŸ¦  bing-covid-nlp

## Description
Classifying Bing COVID-19 queries into Specific vs Generic using NLP

This project focuses on classifying COVID-19â€“related search queries from Bing into two categories:

> Specific queries â†’ contain references to particular locations, timelines, or entities

> Generic queries â†’ general questions about COVID-19 symptoms, spread, prevention, etc.

The goal is to use Natural Language Processing (NLP) techniques to automatically detect the â€œspecificityâ€ of a query.
This helps search engines, health organizations, and information systems understand public needs more accurately.

## ğŸ“Œ Project Goals

* Build a clean and reproducible NLP pipeline
* Classify queries into Specific vs Generic
* Compare machine learning models (Logistic Regression, SVM, Random Forest, XGBoost)
* Evaluate deep learning models (LSTM / BERT)
* Analyze which keywords strongly indicate specific vs generic intent
* Provide explainability (e.g., SHAP, LIME)

## ğŸ“ Dataset

BingCoronavirusQuerySet 

Includes anonymized COVID-19â€“related queries collected during the pandemic.

Each row typically contains:

- Query â€” text entered by a user
- Label â€” specific or generic

## ğŸ—ï¸ Project Structure
```
bing-covid-nlp/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploration.ipynb
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb
â”‚   â”œâ”€â”€ 03_baseline_models.ipynb
â”‚   â””â”€â”€ 04_bert_model.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ train_baseline.py
â”‚   â”œâ”€â”€ train_bert.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ saved_models/
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ metrics.json
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â””â”€â”€ feature_importance.png
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”§ Installation

```
git clone https://github.com/mehdi_1171/bing-covid-nlp
cd bing-covid-nlp
pip install -r requirements.txt

```

## ğŸ§¹ Preprocessing Steps

- Text normalization
- Lowercasing
- Removing URLs, numbers, stopwords
- Lemmatization
- TF-IDF vectorization or tokenization (for deep learning)

## ğŸ¤– Models Used
- Baseline ML Models:
    - Logistic Regression 
    - SVM
    - Random Forest
    - Naive Bayes
    - XGBoost

- Deep Learning Models

    - LSTM with pretrained embeddings 
    - BERT (bert-base-uncased)


## ğŸ“Š Evaluation Metrics

- Accuracy
- Precision / Recall / F1-score
- ROC-AUC
- Confusion Matrix

## ğŸ“ Results
```
_____________________________________
|       Model         |  Accuracy   |
_____________________________________
| Logistic Regression |	  0.89      |
_____________________________________
|        SVM          |   0.90      |
_____________________________________
|       BERT	      |   0.95      |    
_____________________________________
```

## ğŸ§  Explainability

To understand why a query is predicted as Specific vs Generic:

- SHAP (feature influence)
- LIME (local explanations)


## ğŸš€ How to Run

- Train baseline model:
```
python src/train_baseline.py
```

- Train BERT model:
```
python src/train_bert.py

```

ğŸ§© Future Improvements

- Add more granular classification (e.g., symptoms / prevention / statistics)
- Deploy model as an API (FastAPI / Flask)
- Create a simple web interface
- Add data augmentation for short queries

ğŸ“œ License

MIT License